{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "- Why are you using machine learning rather than a simpler approach?\n",
    "- What is it about the problem/data that is suitable for logistic regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- One-hot-encoding here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "- Pattern of error?\n",
    "- Consider the business problem when choosing features\n",
    "- Cannabilize the violin plot function in here for EDA:\n",
    "  - 41-classification_workflow-completed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIVE! Eyes on the Prize!\n",
    "- Predictive Findings:\n",
    "  - How well your model is able to predict target\n",
    "  - Which features are most important to model\n",
    "- Predictive Recommendations:\n",
    "  - Context and situation where predictions would be useful\n",
    "  - Suggest to data engineers how data can be transformed upon ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Iterative Modeling Process:\n",
    "1. Dummy model\n",
    "2. Evaluate on appropriate classification metrics (bias/variance?)\n",
    "3. Proceed to next model (provide justification!)\n",
    "- Ultimate Goal is to Create a classifier that beats dummy model - doesn't have to be perfect\n",
    "  - When your model isn't improving and you've tuned a couple, you can stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Modeling Process us Cross-Validation:\n",
    "### Process below is from this lecture (also has lots on how to appropriately use Regularization): ***36-Regularization_Lecture-completed.ipynb***\n",
    "\n",
    "So, now our modeling process has an added step: cross-validation.\n",
    "\n",
    "1. Get Data\n",
    "2. EDA\n",
    "3. Cleaning\n",
    "4. Feature Engineering\n",
    "5. Train/Test split\n",
    "6. Model training using `train` split\n",
    "7. Cross Validation (Once you are happy with the model, then do step 8)\n",
    "8. Model testing using `test` split\n",
    "\n",
    "Please note, this is **NOT** a linear process.\n",
    "\n",
    "You will repeat steps 3 through 7 many times. \n",
    "\n",
    "You only use the `test` split when you are satisfied of your model's performance as judged by the cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See this lecture for example of entire logistic regression (classification) workflow: \n",
    "### **41-classification_workflow-completed.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Viable Product:\n",
    "- Fit the transformer on the training data and use it to transform both the train and test\n",
    "- Never fit anything to test\n",
    "- Don't use target as a feature or a numeric target\n",
    "- Don't use regularization on a model that is overfitting\n",
    "- Report the model's performance on the TEST data, not the training data.\n",
    "- Use at least 2 types of scikit-learn models (logistic regression and ridge-lasso good enough?)\n",
    "- Tune at least 1 hyperparameter in a justifiable way without any major errors\n",
    "- Focus on specific metrics that are important to business case (not just displaying `classification_report` and/or confusion matrix -- you wouldn't want to try and discuss ALL evaluation metrics, and you also wouldn't want to just display the metrics without discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "- Evaluate based on TEST data\n",
    "- Bias/Variance - overfitting vs. underfitting\n",
    "- Recall\n",
    "- Precision\n",
    "- Accuracy\n",
    "- MSE (better than R-squared for explaining to stakeholders)\n",
    "- Confusion Matrix\n",
    "- `classification_report`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning! Make justifiable changes in successive models:\n",
    "- DO NOT reduce regularization on a model that is overfitting.\n",
    "  - Regularization parameter (lambda) penalizes all the parameters except intercept so that model generalizes the data and wonâ€™t overfit.\n",
    "- Threshold: Calculate from ROC Curve:\n",
    "  - https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "- Tune Hyperparameters and Grid Search:\n",
    "  - https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "- Manipulate target?\n",
    "- Maximize validation scores\n",
    "  - As a validation score, accuracy is inappropriate for imbalanced classification problems\n",
    "  - Use precision, recall, F-Measure for imbalanced classification: https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "- Cross validation: https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/\n",
    "- Consider Bias/Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "- After refining models, provide 1-3 paragraphs discussing final model and at least 1 overall model metric\n",
    "- Model Limitations Discussion:\n",
    "  - Records/Instances where model performance was worse (Question: Does this mean an individual row?)\n",
    "  - If used in production, what kinds of problems would pop up?\n",
    "  - Connect metrics to real-world implications \n",
    "    - What should stakeholders do with this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional:\n",
    "- Cross Validation\n",
    "- Feature Engineering\n",
    "- Pipelines\n",
    "  - Only use pipelines later if you have time\n",
    "  - Pipelines help prevent data leakage\n",
    "  - They transform the test set exactly how train was transformed\n",
    "  - Pipelines are the best-practice approach to data preparation that avoids leakage, but they can get complicated very quickly. We therefore do not recommend that you use pipelines in your initial modeling approach, but rather that you refactor to use pipelines if you have time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
